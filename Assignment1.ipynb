{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301633e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#TO Encode, Scale and split data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#The Models we are going to use\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#To make a print_score function\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad63551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and preprocess it\n",
    "dataset =  pd.read_csv(\"agaricus-lepiota.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef74403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.head()\n",
    "#dataset.info() #Shows that every row has non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96b0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wanted to see a heatmap of the correletaiom between the different values and collum.\n",
    "#This will show the  most indicative features in the dataset and the most domentating feutures. \n",
    "#corr = dataset.corr()\n",
    "#sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa3c722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to drop some coloms because of the logical rules in the dataset.\n",
    "#This is features that are the most indicative, and we would therefore drop these before running the models. \n",
    "dataset = dataset.drop(columns='p.1') #Odor\n",
    "dataset = dataset.drop(columns='k.1') #Spore-print-color\n",
    "dataset = dataset.drop(columns='u') #Habitant\n",
    "dataset = dataset.drop(columns='s.3') #Population\n",
    "dataset = dataset.drop(columns='s.1')#Above\n",
    "dataset = dataset.drop(columns='s.2')#below\n",
    "dataset = dataset.drop(columns='p.3')#ring-type\n",
    "dataset = dataset.drop(columns='e.1')#stalk-root\n",
    "dataset = dataset.drop(columns='k')#gill-color\n",
    "dataset = dataset.drop(columns='t')#bruises?\n",
    "dataset = dataset.drop(columns='c')#gill-spacing\n",
    "dataset = dataset.drop(columns='n.1')#gill-size\n",
    "dataset = dataset.drop(columns='e')#stalk-shape\n",
    "dataset = dataset.drop(columns='n')#cap-color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f38394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Label Encoding, so that the cells contain corresponding number for the character, and replaces it. \n",
    "Encoder = LabelEncoder() \n",
    "for col in dataset.columns:\n",
    "    dataset[col] = Encoder.fit_transform(dataset[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d0054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'p'/class is the column that decides if the mushroom is edible or poisounus.  \n",
    "X=dataset.drop('p',axis=1) #Predictors 'p'=\"class\", This is what we are trying to predict.\n",
    "y=dataset['p'] #Response, the data we have to work with, is then the rest of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92d0ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b73bd105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f97acb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to use KNN, DecisionTreeclassifier and RandomForrestClassifier because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10a2b215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(oob_score=True)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNN()\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=5, criterion=\"entropy\")\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "#RandomForrestClassifier\n",
    "rfc = RandomForestClassifier(oob_score=True)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41f2a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to print a classification report for the different models.\n",
    "def print_score(classifier,X_train,y_train,X_test,y_test, name):\n",
    "    print(\"Training results for\", name, \":\\n\")\n",
    "    print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c61f638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results for KNN :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83      2945\n",
      "           1       0.87      0.72      0.79      2741\n",
      "\n",
      "    accuracy                           0.81      5686\n",
      "   macro avg       0.82      0.81      0.81      5686\n",
      "weighted avg       0.82      0.81      0.81      5686\n",
      "\n",
      "\n",
      "Training results for DTC :\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.52      0.67      2945\n",
      "           1       0.65      0.96      0.78      2741\n",
      "\n",
      "    accuracy                           0.73      5686\n",
      "   macro avg       0.79      0.74      0.72      5686\n",
      "weighted avg       0.80      0.73      0.72      5686\n",
      "\n",
      "\n",
      "Training results for RFC :\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      2945\n",
      "           1       0.80      0.88      0.84      2741\n",
      "\n",
      "    accuracy                           0.83      5686\n",
      "   macro avg       0.84      0.84      0.83      5686\n",
      "weighted avg       0.84      0.83      0.83      5686\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(knn,X_train,y_train,X_test,y_test,\"KNN\")\n",
    "print_score(dtc,X_train,y_train,X_test,y_test,\"DTC\")\n",
    "print_score(rfc,X_train,y_train,X_test,y_test,\"RFC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b8923",
   "metadata": {},
   "source": [
    "c) Our best performing model is the RandomForrest classifier, as the average score is 83%. We would still not trust the model as there is still 20% that is wrong, and that can be dangerous. The dataset was originally sufficient because it would give a score of 100% for all three models. To really test the models we would need to decrease the numbers of colums and avoid the logical rules of the dataset. \n",
    "\n",
    "The decision tree classifier is a tree-like model that splits the data based on the most significant feature until a prediction can be made.\n",
    "We think the random forrest classifier perform best because it is an ensemble of decision trees, where each tree makes a prediction and the final prediction is made by combining the predictions of all trees. Therefore, the RFC is even more precise than the dtc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
