{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I will have a couple of tasks to performe in my process of sentiment analysis:\n",
    "## Sentiment Analysis process:\n",
    "    1.  Analyze the Data and understand how I should handle it.\n",
    "    2.  Load the necessary data and filter out redundant information.\n",
    "    3.  Handle the data and prepare it such that it fits the models I will be using. This step will contain a couple of subprocess:\n",
    "        a. Tokenizing the data.\n",
    "        b. Sanitizing the data by removing all the stopwords, punctuation and numbers that will only reduce the performance of the Sentiment Analysis model. \n",
    "        c. Lemmatizing the data in order to make it more understandable for the model.\n",
    "        d. Vectorizing in order to fit the data to an graph that I will be using when performing the real Sentiment Analysis. \n",
    "    4. Finding the most optimal algorithm.\n",
    "        a. I will have to try different algorithms with cross validation and use their results in order to find which one fits the data the best. \n",
    "        b. In addition I will need to find the most optimal parameters. I will see later how I choose to search for them. \n",
    "    5. Training the model with the preprocessed data.\n",
    "    6. Testing the model with the preprocessed data. \n",
    "\n",
    "And then I will be evaluating the data, to see if my score is sufficient or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# This is for the sentimental analysis more exactly for comparing the different algorithms with eachother. \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data that I will handle:\n",
    "def data_analysys(file_name) -> list:\n",
    "    message_data = []\n",
    "    with open(f'3class/{file_name}', \"r\") as training_data:\n",
    "        keys_in_data = set()\n",
    "        train_array = json.load(training_data)\n",
    "\n",
    "        print(\"The size of the data is:\", len(train_array))\n",
    "\n",
    "        for item in train_array:\n",
    "            keys = item.keys()\n",
    "            keys_in_data.update(keys)\n",
    "        training_data.close() \n",
    "            \n",
    "    print(keys_in_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function defined above I am able to extracts all the different categories from the dataset, without needing to do manual looking into the .json document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the data is: 7973\n",
      "{'text', 'label', 'sent_id'}\n"
     ]
    }
   ],
   "source": [
    "data_analysys(\"train.json\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will only be using the text and label part of this data so I continue in defining a function that will extracts exactly that part of the data. \n",
    "\"data_reading()\" extracts the data from the json file provided in a structured way: [\"sentance\", \"sentiment\"]. I consider the data not to complex so I decided to not bother with visualizing the data to much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reading(file_name) -> list:\n",
    "    # Since I don't want to overfit, I must 75% only use 75% of the data. \n",
    "    # So I have to find how much of the data is suppose to go as train data and how much of the data should go to validation:  \n",
    "    message_data = []\n",
    "    with open(f'3class/{file_name}', \"r\") as data:\n",
    "        data_array = json.load(data)\n",
    "\n",
    "        # Simply implement the division here:\n",
    "        full = len(data_array)\n",
    "        seventy_prcnt = 0.75 * full\n",
    "        rest_prcnt = full-seventy_prcnt\n",
    "\n",
    "        #data_array is an array with dictionaries, each having three elements each. \n",
    "        for category in data_array:\n",
    "            #category is the index of the list with the three elements: sent_id, text, label\n",
    "            message_data.append([category[\"text\"], category[\"label\"]]) # This appends [text, label] to the array.\n",
    "\n",
    "        data.close() \n",
    "            \n",
    "    test = message_data[:round(seventy_prcnt)]\n",
    "    validate = message_data[-round(rest_prcnt):]\n",
    "\n",
    "    return test, validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later when working with this project I struggled with removing the non-alphabetic characters from the dataset and more so to do that in an efficient way. Without needing to access the data unnecessary many times. So I came to a consensus with myself in doing tokenization, sanitazation and lemmatization in one function and name that function: preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    sentances = []\n",
    "    sentiments = []\n",
    "    lemmatizer = spacy.load(\"nb_core_news_sm\") # Norwegian lemmatization model. \n",
    "    sentance = \" \"\n",
    "\n",
    "    for chunk in data: # chunk is a index in data that both contains the sentance and it's sentiment. Not edited. \n",
    "        that_text_part_of_the_chunk, that_sentiment_part_of_the_chunk = chunk[0], chunk[1]\n",
    "        \n",
    "        # pre_lemmed is a sentance that is put in the spacy format, I do this in order to later be able to use spacy different attributes to sanitize my data.\n",
    "        pre_lemmed = lemmatizer(that_text_part_of_the_chunk) \n",
    "        lemmed = []\n",
    "\n",
    "        for root_word in pre_lemmed:\n",
    "            # Here I \"manually\" sanitize the data by checking if the current word/lemma is anything else but a string. \n",
    "            # I found this method much more accurate then relying on a symbol library and comparing the current word with each symbol for each word that I processed. \n",
    "            if (not root_word.is_punct\n",
    "                    and not root_word.is_currency \n",
    "                    and not root_word.is_digit\n",
    "                    and not root_word.is_space\n",
    "                    and not root_word.is_stop\n",
    "                    and not root_word.like_num):\n",
    "                \n",
    "                lemmaRoot = root_word.lemma_ # Reducing the word to it's original lemma:\n",
    "                lower_lemmaRoot = lemmaRoot.lower() # Lowercase to make it more efficient for the machine to work with.\n",
    "                lemmed.append(lower_lemmaRoot) #Append it to the list that will represent a sentance.\n",
    "                \n",
    "        # When done with creating the list that will be a sentance. Join each index into a sentance:\n",
    "        sentance = ' '.join(lemmed)\n",
    "        sentances.append(sentance) #And create a list of sentances\n",
    "        sentiments.append(that_sentiment_part_of_the_chunk) \n",
    "\n",
    "    return sentances, sentiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I figured that some sentances may consist of only stopwords so I will need a function where I can check that the sentance list is not empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(sentances, sentiments):\n",
    "    fixed_sentances = []\n",
    "    fixed_sentiments = []\n",
    "    \n",
    "    for sentance_index in range(len(sentances)):\n",
    "\n",
    "        if len(sentances[sentance_index]) != 0:\n",
    "            fixed_sentances.append(sentances[sentance_index])\n",
    "            fixed_sentiments.append(sentiments[sentance_index])\n",
    "\n",
    "    return fixed_sentances, fixed_sentiments\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the function I was struggling with accessing the data in an efficient way. I wanted to divide: the sanitizing, tokenizing, lemmatizing and vectorizing of the data. But I quickly figured that it would have cost a lot of computation time since I would have to access the same data multiple times in nested for loops. So instead I created preprocessing() which did sanitizing with tokenizing, and then lemmatized the sentances in one go. In addition to that. This function divides the preprocessed data into sentances and sentiments, which are perfectly prepared to be vectorized.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the data through the preprocessing() function my data can be easily vectorized since the function separates sentances and theirs sentiments:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I performe the vectorization using CountVectorizer() & LabelEncoder() because that seemed most trivial of every other way that I stumbled upon. I considered if I should use TF-IDF or Bag-of-Words vectorization. But I concluded with TF-IDF seemed to be more fit for bigger datas with variaty of lenghts and inputs. Meanwhile here I have tree prepared documents with the same format which will not vary as much as data from the real world would. \n",
    "And my implementation is shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectornihilation(x_axis, y_axis):\n",
    "    vectorizer = CountVectorizer()\n",
    "    labelizer = LabelEncoder()\n",
    "    X_matrix = vectorizer.fit_transform(x_axis)\n",
    "    Y_matrix = labelizer.fit_transform(y_axis)\n",
    "\n",
    "    return X_matrix, Y_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this function gives me a matrix as the x vectors and a numpy array with vector representations of the labels as y."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have preprocessed data I can use that tougheter with the GridSearchCV() method in order to find the best algorithm and the best parameters for that algorithm to predict the sentiments of the sentances. \n",
    "\n",
    "I do that by implementing exhoustive search on the parameters for each of the algorithms.\n",
    "\n",
    "I choose these tree algorithms because I have read that those are most often tried when performing sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesScore(x_ax, y_ax):\n",
    "    hypeOmeters = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "\n",
    "    grid = GridSearchCV(GaussianNB(), hypeOmeters, cv=5)\n",
    "    grid.fit(X=x_ax, y=y_ax)\n",
    "    \n",
    "    optimal_param = grid.best_params_['var_smoothing']\n",
    "    \n",
    "    return optimal_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionScore(x_ax, y_ax):\n",
    "    C_meters = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "    grid = GridSearchCV(LogisticRegression(), C_meters, cv=5) \n",
    "    grid.fit(X=x_ax, y=y_ax)\n",
    "    \n",
    "    optimal_param = grid.best_params_['C']\n",
    "\n",
    "    return optimal_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecissionTreeScore(x_ax, y_ax):\n",
    "    grido_meters = { 'max_depth': [2, 4, 6, 8, 10], 'min_samples_split': [2, 5, 10, 20] }\n",
    "    grid = GridSearchCV(DecisionTreeClassifier(), grido_meters, cv=5)\n",
    "\n",
    "    grid.fit(X=x_ax, y=y_ax)\n",
    "\n",
    "    optimal_p1, optimal_p2 = grid.best_params_['max_depth'], grid.best_params_['min_samples_split']\n",
    "\n",
    "    return optimal_p1, optimal_p2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here I will create a function that will test the different algorithm with their best parameters and present the best algorithm with it's best parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation:\n",
    "def OptimalSentimentAnalysis(file):\n",
    "    data, validation = data_reading(file) #TODO: Remove the limit.\n",
    "    sentances, sentiments = preprocessing(data= data)\n",
    "    sentances_secured, sentiments_secured = check(sentances, sentiments) \n",
    "    x_axis, y_axis = vectornihilation(sentances_secured, sentiments_secured)\n",
    "    densed_X = x_axis.toarray() # I had to convert the matrix to danse array in order to process it\n",
    "\n",
    "    Logistic_param = LogisticRegressionScore(x_ax=densed_X, y_ax=y_axis)\n",
    "    Naive_param = NaiveBayesScore(x_ax=densed_X, y_ax=y_axis)\n",
    "    DecTree_depth_param, DecTree_sample_split_param = DecissionTreeScore(x_ax=densed_X, y_ax=y_axis)\n",
    "\n",
    "    Cross_Naive_score = cross_val_score(GaussianNB(var_smoothing=Naive_param), densed_X, y_axis)\n",
    "    Cross_Logistic_score = cross_val_score(LogisticRegression(C=Logistic_param), densed_X, y_axis)\n",
    "    Cross_DecTree_score = cross_val_score(DecisionTreeClassifier(max_depth=DecTree_depth_param, min_samples_split=DecTree_sample_split_param), densed_X, y_axis)\n",
    "    \n",
    "    print(f'Naive Bayes algorithm scored: {Cross_Naive_score.mean()}')\n",
    "    print(f'LogisticRegression algorithm scored: {Cross_Logistic_score.mean()}')\n",
    "    print(f'Decission Tree scored: {Cross_DecTree_score.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m OptimalSentimentAnalysis(\u001b[39m\"\u001b[39;49m\u001b[39mtrain.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mOptimalSentimentAnalysis\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mOptimalSentimentAnalysis\u001b[39m(file):\n\u001b[0;32m      3\u001b[0m     data, validation \u001b[39m=\u001b[39m data_reading(file) \u001b[39m#TODO: Remove the limit.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     sentances, sentiments \u001b[39m=\u001b[39m preprocessing(data\u001b[39m=\u001b[39;49m data)\n\u001b[0;32m      5\u001b[0m     sentances_secured, sentiments_secured \u001b[39m=\u001b[39m check(sentances, sentiments) \n\u001b[0;32m      6\u001b[0m     x_axis, y_axis \u001b[39m=\u001b[39m vectornihilation(sentances_secured, sentiments_secured)\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m that_text_part_of_the_chunk, that_sentiment_part_of_the_chunk \u001b[39m=\u001b[39m chunk[\u001b[39m0\u001b[39m], chunk[\u001b[39m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[39m# pre_lemmed is a sentance that is put in the spacy format, I do this in order to later be able to use spacy different attributes to sanitize my data.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m pre_lemmed \u001b[39m=\u001b[39m lemmatizer(that_text_part_of_the_chunk) \n\u001b[0;32m     12\u001b[0m lemmed \u001b[39m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m root_word \u001b[39min\u001b[39;00m pre_lemmed:\n\u001b[0;32m     15\u001b[0m     \u001b[39m# Here I \"manually\" sanitize the data by checking if the current word/lemma is anything else but a string. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39m# I found this method much more accurate then relying on a symbol library and comparing the current word with each symbol for each word that I processed. \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\transition_parser.pyx:253\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\transition_parser.pyx:274\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\ml\\tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 33\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[0;32m     34\u001b[0m         X,\n\u001b[0;32m     35\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[0;32m     36\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     37\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[0;32m     38\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     39\u001b[0m     )\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\ml\\parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\linear.py:39\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     37\u001b[0m W \u001b[39m=\u001b[39m cast(Floats2d, model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     38\u001b[0m b \u001b[39m=\u001b[39m cast(Floats1d, model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m---> 39\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     40\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m b\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OptimalSentimentAnalysis(\"train.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I quickly explain what different parameters does in the different models:\n",
    "var_smoothing variable chooses how smoothly the variance should be distributed in the Naive Bayes distribution model.\n",
    "\n",
    "C is a value that regularates the complexity of the Logistic Regregression model. What I try to find is the maximum Likelyhood parameter. Which is the parameter that gives me the optimal values for the likelyhood of getting Y given an X.\n",
    "\n",
    "max_depth is the maximum depth of the decision tree. A decision tree can become very complex and can overfit the training data if it is allowed to grow too deep. So, setting a maximum depth for the tree can help to prevent overfitting and improve generalization to new data.\n",
    "\n",
    "min_samples_split chooses how small the decision groups can be, preventing this parameter from being to small can help in preventing overfitting by creating a more general model that fits more data. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5a8d3d6fa4a8c42668baa170a0b6198815339ab86606f8fa92c48503601c5fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
