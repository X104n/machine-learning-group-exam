{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc4x0R-UpLKx"
      },
      "source": [
        "In this task I will have a couple of tasks to performe in my process of sentiment analysis:\n",
        "## Sentiment Analysis process:\n",
        "    1.  Analyze the Data and understand how I should handle it.\n",
        "    2.  Load the necessary data and filter out redundant information.\n",
        "    3.  Handle the data and prepare it such that it fits the models I will be using. This step will contain a couple of subprocess:\n",
        "        a. Tokenizing the data.\n",
        "        b. Sanitizing the data by removing all the stopwords, punctuation and numbers that will only reduce the performance of the Sentiment Analysis model. \n",
        "        c. Lemmatizing the data in order to make it more understandable for the model.\n",
        "        d. Vectorizing in order to fit the data to an graph that I will be using when performing the real Sentiment Analysis. \n",
        "    4. Finding the most optimal algorithm.\n",
        "        a. I will have to try different algorithms with cross validation and use their results in order to find which one fits the data the best. \n",
        "        b. In addition I will need to find the most optimal parameters. I will see later how I choose to search for them. \n",
        "    5. Training the model with the preprocessed data.\n",
        "    6. Testing the model with the preprocessed data. \n",
        "\n",
        "And then I will be evaluating the data, to see if my score is sufficient or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKs5KxeSpLK3",
        "outputId": "fa7a8385-c634-431b-c74d-d3f1069fb584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz (16.1 MB)\n",
            "                                              0.0/16.1 MB ? eta -:--:--\n",
            "                                              0.1/16.1 MB 4.1 MB/s eta 0:00:04\n",
            "                                              0.1/16.1 MB 1.6 MB/s eta 0:00:11\n",
            "                                              0.2/16.1 MB 2.1 MB/s eta 0:00:08\n",
            "                                              0.2/16.1 MB 2.1 MB/s eta 0:00:08\n",
            "     -                                        0.5/16.1 MB 2.2 MB/s eta 0:00:08\n",
            "     --                                       0.8/16.1 MB 3.1 MB/s eta 0:00:05\n",
            "     ---                                      1.3/16.1 MB 4.0 MB/s eta 0:00:04\n",
            "     ----                                     1.7/16.1 MB 4.8 MB/s eta 0:00:04\n",
            "     -----                                    2.0/16.1 MB 5.0 MB/s eta 0:00:03\n",
            "     ------                                   2.5/16.1 MB 5.6 MB/s eta 0:00:03\n",
            "     --------                                 3.2/16.1 MB 6.5 MB/s eta 0:00:02\n",
            "     ---------                                4.0/16.1 MB 7.3 MB/s eta 0:00:02\n",
            "     -----------                              4.8/16.1 MB 8.1 MB/s eta 0:00:02\n",
            "     -------------                            5.6/16.1 MB 8.7 MB/s eta 0:00:02\n",
            "     ----------------                         6.5/16.1 MB 9.4 MB/s eta 0:00:02\n",
            "     -----------------                        7.2/16.1 MB 9.8 MB/s eta 0:00:01\n",
            "     -------------------                      7.9/16.1 MB 10.1 MB/s eta 0:00:01\n",
            "     ---------------------                    8.7/16.1 MB 10.4 MB/s eta 0:00:01\n",
            "     ------------------------                 9.7/16.1 MB 11.1 MB/s eta 0:00:01\n",
            "     -------------------------               10.4/16.1 MB 12.1 MB/s eta 0:00:01\n",
            "     ---------------------------             11.3/16.1 MB 15.2 MB/s eta 0:00:01\n",
            "     -----------------------------           12.3/16.1 MB 17.7 MB/s eta 0:00:01\n",
            "     --------------------------------        13.3/16.1 MB 17.7 MB/s eta 0:00:01\n",
            "     ----------------------------------      14.4/16.1 MB 18.7 MB/s eta 0:00:01\n",
            "     -------------------------------------   15.4/16.1 MB 19.3 MB/s eta 0:00:01\n",
            "     --------------------------------------  16.1/16.1 MB 19.3 MB/s eta 0:00:01\n",
            "     --------------------------------------  16.1/16.1 MB 19.3 MB/s eta 0:00:01\n",
            "     --------------------------------------- 16.1/16.1 MB 15.9 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nb-core-news-sm==3.1.0) (3.1.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (8.0.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.28.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oskar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->nb-core-news-sm==3.1.0) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# This is for the sentimental analysis more exactly for comparing the different algorithms with eachother. \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/nb_core_news_sm-3.1.0/nb_core_news_sm-3.1.0.tar.gz"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.Analyzing the data & understanding its structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MJXjXd5OpLK5"
      },
      "outputs": [],
      "source": [
        "def data_analysys(json_data) -> list:\n",
        "    keys_in_data = set()\n",
        "    print(\"The size of the data is:\", len(json_data))\n",
        "\n",
        "    for item in json_data:\n",
        "        keys = item.keys()\n",
        "        keys_in_data.update(keys)\n",
        "\n",
        "    print(f'Main keys of this data are: {keys_in_data}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will use the function above to analyze the structure of the content of this file. To learn how I should be preprocessing it. And how I can make it more efficient."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell below I simply load the documents from the url's "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace this URL with the raw URL of the file you want to fetch from GitHub\n",
        "train_url = \"https://raw.githubusercontent.com/ltgoslo/norec_sentence/main/binary/train.json\"\n",
        "test_url = \"https://raw.githubusercontent.com/ltgoslo/norec_sentence/main/binary/test.json\"\n",
        "\n",
        "# Fetch the content of the file\n",
        "train_response = requests.get(train_url)\n",
        "test_response = requests.get(test_url)\n",
        "\n",
        "# Raise an exception if there was an error fetching the file\n",
        "train_response.raise_for_status() \n",
        "test_response.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89m2jGBOpLK6",
        "outputId": "7152dc5c-f2a7-4295-a2fc-44d6b854e839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the data is: 3894\n",
            "Main keys of this data are: {'sent_id', 'label', 'text'}\n",
            "The size of the data is: 583\n",
            "Main keys of this data are: {'sent_id', 'label', 'text'}\n"
          ]
        }
      ],
      "source": [
        "# Get the content of the file as a string\n",
        "json_training_data = json.loads(train_response.text)\n",
        "json_test_data = json.loads(test_response.text)\n",
        "seed = 123\n",
        "\n",
        "# This is just for analytic reasons. Not for loading the content:\n",
        "data_analysys(json_training_data)\n",
        "data_analysys(json_test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.Loading the data & filtering out what's redundant"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j0EHx3IBpLK7"
      },
      "source": [
        "As you can see there's an extra, redundant label that I do not have to consider doing my sentiment analysis, and that is the \"sent_id\" label. The date the message was sent is irrelevant therefore I can remove it from my data to do more efficient model training. \n",
        "\n",
        "I will only be using the text and label part of this data so I continue in defining a function that reads only 'label' & 'text' part of the data. \n",
        "\n",
        "\n",
        "\"data_reading()\" extracts the data from the json file provided in a structured way: [\"sentance\", \"sentiment\"]. I consider the data not to complex so I decided to not bother with visualizing the data to much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TgPxQu2HpLK8"
      },
      "outputs": [],
      "source": [
        "def data_reading(json_data) -> list:\n",
        "    # Extract message_data directly using a list comprehension\n",
        "    message_data = [[category[\"text\"], category[\"label\"]] for category in json_data]\n",
        "\n",
        "    return message_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.Handling & Praparing the data for my model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3_Y9pApLK9"
      },
      "source": [
        "Before creating the function I was struggling with accessing the data in an efficient way. I wanted to divide: the sanitizing, tokenizing, lemmatizing and vectorizing of the data. But I quickly figured that it would have cost a lot of computation time since I would have to access the same data multiple times in nested for loops. So instead I created preprocessing() which did sanitizing with tokenizing, and then lemmatized the sentances in one go. In addition to that, this function divides the preprocessed data into sentances and sentiments, which are perfectly prepared to be vectorized. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2PLrdkmFpLK9"
      },
      "outputs": [],
      "source": [
        "def preprocessing(data):\n",
        "    sentences = []\n",
        "    sentiments = []\n",
        "    lemmatizer = spacy.load(\"nb_core_news_sm\")  # Norwegian lemmatization model.\n",
        "\n",
        "    for chunk in data:\n",
        "        text, sentiment = chunk[0], chunk[1]\n",
        "        # Here I tokenize the data, and prepare it to run it through a sanitizer.\n",
        "        pre_lemmatized = lemmatizer(text)\n",
        "\n",
        "\n",
        "        # Here I add the lemmatized lowercase word to the \"lemmatized\" list,\n",
        "        # but only after it passes all the tests, to check if it's a legitimate word.  \n",
        "        lemmatized = [root_word.lemma_.lower() for root_word in pre_lemmatized\n",
        "                      if (not root_word.is_punct\n",
        "                          and not root_word.is_currency\n",
        "                          and not root_word.is_digit\n",
        "                          and not root_word.is_space\n",
        "                          and not root_word.is_stop\n",
        "                          and not root_word.like_num)]\n",
        "\n",
        "\n",
        "        # This if statement checks if the list is empty or not.\n",
        "        # If it is, it simply continues to the next sentance without adding the \n",
        "        # sentiment. That provides a reasurance that there will not be any sentiments\n",
        "        # attached to a empty list.\n",
        "        if len(lemmatized)==0:\n",
        "            continue\n",
        "        else:\n",
        "            # And here I join each processed word to one sentance in order to do\n",
        "            # sentance sentimental analysis.\n",
        "            sentence = ' '.join(lemmatized)\n",
        "            sentences.append(sentence)\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "    return sentences, sentiments\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "os-AnehYpLK-"
      },
      "source": [
        "In the last part of the for loop, where I have that if statement I figured that when filtering out all the stopwords all slang words may be filtered out too. So I needed to add that to not end up with some sentiments being attached to empty lists. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cZA3BJpwpLK_"
      },
      "source": [
        "So I performe the vectorization using CountVectorizer() & LabelEncoder() because that seemed most trivial of every other way that I stumbled upon. I considered if I should use TF-IDF or Bag-of-Words vectorization. But I concluded with TF-IDF seemed to be more fit for bigger datas with variaty of lenghts and inputs. Meanwhile here I have tree prepared documents with the same format which will not vary as much as data from the real world would. \n",
        "And my implementation is shown bellow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_iPKM_IapLK_"
      },
      "outputs": [],
      "source": [
        "def vectornihilation(x_axis, y_axis):\n",
        "    vectorizer = CountVectorizer()\n",
        "    labelizer = LabelEncoder()\n",
        "    X_matrix = vectorizer.fit_transform(x_axis)\n",
        "    Y_matrix = labelizer.fit_transform(y_axis)\n",
        "\n",
        "    return X_matrix, Y_matrix, vectorizer, labelizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luaJf8cspLK_"
      },
      "source": [
        "And this function gives me a matrix as the x vectors and a numpy array with vector representations of the labels as y."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "drzstF1hpLK_"
      },
      "source": [
        "### 4. Finding the optimal algorithm to performe sentance sentimental analysis.\n",
        "\n",
        "Now that I have preprocessed data I can use that tougheter with the GridSearchCV() method in order to find the best algorithm and the best parameters for that algorithm to predict the sentiments of the sentances. \n",
        "\n",
        "I do that by implementing exhoustive search on the parameters for each of the algorithms.\n",
        "\n",
        "I choose these tree algorithms because I have read that those are most often tried when performing sentiment analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uxdYXglvpLLA"
      },
      "outputs": [],
      "source": [
        "def NaiveBayesScore(x_ax, y_ax):\n",
        "    hypeOmeters = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "\n",
        "    grid = GridSearchCV(GaussianNB(), hypeOmeters, cv=5)\n",
        "    grid.fit(X=x_ax, y=y_ax)\n",
        "    \n",
        "    optimal_param = grid.best_params_['var_smoothing']\n",
        "    \n",
        "    return optimal_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n0eqRKwcpLLA"
      },
      "outputs": [],
      "source": [
        "def LogisticRegressionScore(x_ax, y_ax):\n",
        "    C_meters = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
        "\n",
        "    grid = GridSearchCV(LogisticRegression(), C_meters, cv=5) \n",
        "    grid.fit(X=x_ax, y=y_ax)\n",
        "    \n",
        "    optimal_param = grid.best_params_['C']\n",
        "\n",
        "    return optimal_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IRlgG8KapLLA"
      },
      "outputs": [],
      "source": [
        "def DecissionTreeScore(x_ax, y_ax):\n",
        "    grido_meters = { 'max_depth': [2, 4, 6, 8, 10], 'min_samples_split': [2, 5, 10, 20] }\n",
        "    grid = GridSearchCV(DecisionTreeClassifier(), grido_meters, cv=5)\n",
        "\n",
        "    grid.fit(X=x_ax, y=y_ax)\n",
        "\n",
        "    optimal_p1, optimal_p2 = grid.best_params_['max_depth'], grid.best_params_['min_samples_split']\n",
        "\n",
        "    return optimal_p1, optimal_p2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_dXCuBpLLA"
      },
      "source": [
        "And here I will create a function that will test the different algorithm with their best parameters and present the best algorithm with it's best parameter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cYs6uLNGpLLB"
      },
      "outputs": [],
      "source": [
        "# Cross-validation:\n",
        "def OptimalSentimentAnalysis(json_data):\n",
        "    data = data_reading(json_data) #TODO: Remove the limit.\n",
        "    sentances, sentiments = preprocessing(data= data)\n",
        "    x_axis, y_axis, vec, lab = vectornihilation(sentances, sentiments)\n",
        "    densed_X = x_axis.toarray() # I had to convert the matrix to danse array in order to process it\n",
        "\n",
        "    Logistic_param = LogisticRegressionScore(x_ax=densed_X, y_ax=y_axis)\n",
        "    Naive_param = NaiveBayesScore(x_ax=densed_X, y_ax=y_axis)\n",
        "    DecTree_depth_param, DecTree_sample_split_param = DecissionTreeScore(x_ax=densed_X, y_ax=y_axis)\n",
        "\n",
        "    Cross_Naive_score = cross_val_score(GaussianNB(var_smoothing=Naive_param), densed_X, y_axis)\n",
        "    Cross_Logistic_score = cross_val_score(LogisticRegression(C=Logistic_param), densed_X, y_axis)\n",
        "    Cross_DecTree_score = cross_val_score(DecisionTreeClassifier(max_depth=DecTree_depth_param, min_samples_split=DecTree_sample_split_param), densed_X, y_axis)\n",
        "    \n",
        "    print(f'Naive Bayes algorithm scored: {Cross_Naive_score.mean()}')\n",
        "    print(f'LogisticRegression algorithm scored: {Cross_Logistic_score.mean()}')\n",
        "    print(f'Decission Tree scored: {Cross_DecTree_score.mean()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSqNe_2tpLLB",
        "outputId": "80941706-155e-43e1-8105-aa5951da99da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes algorithm scored: 0.582878949118723\n",
            "LogisticRegression algorithm scored: 0.7079115397406053\n",
            "Decission Tree scored: 0.6916704356501497\n"
          ]
        }
      ],
      "source": [
        "OptimalSentimentAnalysis(json_training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mp06kekpLLB"
      },
      "source": [
        "Here I quickly explain what different parameters does in the different models:\n",
        "var_smoothing variable chooses how smoothly the variance should be distributed in the Naive Bayes distribution model.\n",
        "\n",
        "C is a value that regularates the complexity of the Logistic Regregression model. What I try to find is the maximum Likelyhood parameter. Which is the parameter that gives me the optimal values for the likelyhood of getting Y given an X.\n",
        "\n",
        "max_depth is the maximum depth of the decision tree. A decision tree can become very complex and can overfit the training data if it is allowed to grow too deep. So, setting a maximum depth for the tree can help to prevent overfitting and improve generalization to new data.\n",
        "\n",
        "min_samples_split chooses how small the decision groups can be, preventing this parameter from being to small can help in preventing overfitting by creating a more general model that fits more data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILymt7WPpLLB"
      },
      "source": [
        "And now I that I have found that Logistic Regression is the model that I am supposed to use I can validate the accuracy of the model before testing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFg0DXigpLLC"
      },
      "outputs": [],
      "source": [
        "def LogisticSentimentAnalysisRegression_trainANDValidate(training_x, training_y, validating_x, validating_y, module):\n",
        "    # The evaluation:\n",
        "    module.fit(training_x, training_y)\n",
        "    \n",
        "    validity_responses = module.predict(validating_x)\n",
        "    \n",
        "    accuracy = accuracy_score(validating_y, validity_responses)\n",
        "    report = classification_report(validating_y, validity_responses)\n",
        "\n",
        "    print(f\"{module}:\\nAccuracy: {accuracy}\\n{report}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7it30oRpLLC",
        "outputId": "1d139593-4899-4488-f3fa-d10b7a98ddf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7152658662092625\n"
          ]
        }
      ],
      "source": [
        "def performance():\n",
        "    LogiReg = LogisticRegression()  # Instantiating the model.\n",
        "\n",
        "    train_sentances, train_sentiments = preprocessing(data_reading(json_training_data))\n",
        "\n",
        "    sentance_vectors_x, sentiment_vectors_y, vectorizer, labelizer = vectornihilation(train_sentances, train_sentiments)\n",
        "    fixed_SV_x = sentance_vectors_x.toarray()  # Convert the matrix to dense array to process it\n",
        "\n",
        "    valid_sentances, valid_sentiments = preprocessing(data_reading(json_test_data))\n",
        "    \n",
        "    # Use the same vectorizer and labelizer for transforming validation data\n",
        "    validSentancesVectors_x = vectorizer.transform(valid_sentances)\n",
        "    validSentimentVectors_Y = labelizer.transform(valid_sentiments)\n",
        "    \n",
        "    fixed_ValidSV_x = validSentancesVectors_x.toarray()\n",
        "\n",
        "    LogisticSentimentAnalysisRegression_trainANDValidate(fixed_SV_x, sentiment_vectors_y, fixed_ValidSV_x, validSentimentVectors_Y, LogiReg)\n",
        "\n",
        "performance()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d5a8d3d6fa4a8c42668baa170a0b6198815339ab86606f8fa92c48503601c5fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
