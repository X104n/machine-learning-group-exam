{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x25a4d0864d0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "category_index_manual = 8\n",
    "n_val = 5000\n",
    "\n",
    "data_path_manual = '/cifar-10-batches-py'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for loading the Cifar10 dataset.\n",
    "\n",
    "The method will have to be run twice. The first time we load it as tensors and split the set between training, validation, and testing. We only want to identify one type of image (ships), so we get the labels of all the images and set all to \"false\" if they are not ships.\n",
    "\n",
    "After running the method for the first time we get create a normalizer from the std and mean of the images. The method is then ran for a second time with the normalizer as the preprocessor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:21: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  train_labels = np.array(transformed_cifar10_train_val)\n",
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_labels = np.array(transformed_cifar10_train_val)\n",
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:22: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  train_labels = np.array(train_labels == category_index).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:25: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  test_labels = np.array(transformed_cifar10_test)\n",
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_labels = np.array(transformed_cifar10_test)\n",
      "C:\\Users\\osten\\AppData\\Local\\Temp\\ipykernel_24708\\3165530937.py:26: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  test_labels = np.array(test_labels == category_index).astype(int)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(n_val, category_index, data_path, preprocessor):\n",
    "    transformed_cifar10_train_val = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform = preprocessor\n",
    "    )\n",
    "\n",
    "    transformed_cifar10_test = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform = preprocessor\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    n_train = len(transformed_cifar10_train_val)-n_val\n",
    "\n",
    "    transformed_cifar10_train_split, transformed_cifar10_val_split = random_split(\n",
    "        transformed_cifar10_train_val,\n",
    "        [n_train, n_val],\n",
    "\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(transformed_cifar10_train_split))\n",
    "    print(\"Size of the validation dataset:   \", len(transformed_cifar10_val_split))\n",
    "    print(\"Size of the test dataset:         \", len(transformed_cifar10_test))\n",
    "\n",
    "load_dataset(n_val, category_index_manual, data_path_manual, preprocessor=transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels = np.array(transformed_cifar10_train_val)\n",
    "train_labels = np.array(train_labels == category_index).astype(int)\n",
    "\n",
    "test_labels = np.array(transformed_cifar10_test)\n",
    "test_labels = np.array(test_labels == category_index).astype(int)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a normalizer for the dataset around the mean and standard deviation of the 3 dimensions (height, width channel (color)).\n",
    "\n",
    "Loading the dataset and applying the composition of transforms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in transformed_cifar10_train_val])\n",
    "\n",
    "normalizer = transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choosing a pre-trained CNN model. We choose ResNet18, which is not trained Cifar-10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last layer for binary classification\n",
    "# Legg til markdown om at vi løser det som binary classification\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,1)\n",
    "\n",
    "# \"Remember to use a suitable loss function like Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss) and an optimizer like Adam or SGD for training.\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Legge til spørsmål om vi burde bruke SGD, Adam, RMSprop eller noe annet\n",
    "# Legge til spørsmål om vi burde endre på learning raten, eller i alle fall forklare hvorfor vi bruker den vi bruker\n",
    "optimizer = optim.adam(model.parameters(),  lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({3: 4504,\n         4: 4510,\n         2: 4504,\n         1: 4471,\n         9: 4520,\n         8: 4506,\n         0: 4513,\n         6: 4517,\n         7: 4505,\n         5: 4450})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([label for _, label in cifar10_train])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.stack([img for img, _ in tensor_cifar10_train_val])\n",
    "print(imgs.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a normalizer for the dataset around the mean and standard deviation of the 3 dimensions (height, width channel (color))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "normalizer = transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the dataset and applying the composition of transforms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at plotting the normalized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[154 177 187]\n",
      "  [126 137 136]\n",
      "  [105 104  95]\n",
      "  ...\n",
      "  [ 91  95  71]\n",
      "  [ 87  90  71]\n",
      "  [ 79  81  70]]\n",
      "\n",
      " [[140 160 169]\n",
      "  [145 153 154]\n",
      "  [125 125 118]\n",
      "  ...\n",
      "  [ 96  99  78]\n",
      "  [ 77  80  62]\n",
      "  [ 71  73  61]]\n",
      "\n",
      " [[140 155 164]\n",
      "  [139 146 149]\n",
      "  [115 115 112]\n",
      "  ...\n",
      "  [ 79  82  64]\n",
      "  [ 68  70  55]\n",
      "  [ 67  69  55]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[175 167 166]\n",
      "  [156 154 160]\n",
      "  [154 160 170]\n",
      "  ...\n",
      "  [ 42  34  36]\n",
      "  [ 61  53  57]\n",
      "  [ 93  83  91]]\n",
      "\n",
      " [[165 154 128]\n",
      "  [156 152 130]\n",
      "  [159 161 142]\n",
      "  ...\n",
      "  [103  93  96]\n",
      "  [123 114 120]\n",
      "  [131 121 131]]\n",
      "\n",
      " [[163 148 120]\n",
      "  [158 148 122]\n",
      "  [163 156 133]\n",
      "  ...\n",
      "  [143 133 139]\n",
      "  [143 134 142]\n",
      "  [143 133 144]]]\n",
      "halla\n",
      "<memory at 0x0000025A4FB4FD60>\n"
     ]
    }
   ],
   "source": [
    "img_t = tensor_cifar10_train_val.data[1]\n",
    "print(img_t)\n",
    "print(\"halla\")\n",
    "print(img_t.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /cifar-10-batches-py\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(tensor_cifar10_train_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24708\\579809553.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_t\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
