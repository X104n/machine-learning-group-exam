{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "category_index = 8\n",
    "n_val = 5000\n",
    "\n",
    "data_path = '/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Function for loading the Cifar10 dataset.\n",
    "\n",
    "The method will have to be run twice.\n",
    "After running the method for the first time we get create a normalizer from the std and mean of the images. The method is then ran for a second time with the normalizer as the preprocessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading the CIFAR-10 dataset as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transformed_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stacking the set of images into a single tensor. We then create a normalizer for the dataset around the mean and standard deviation of the 3 dimensions (height, width channel (color))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in transformed_cifar10_train_val])\n",
    "\n",
    "normalizer = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading the dataset as tensors for training+validation and testing. This time we apply the composition of transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_cifar10_train_val = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform = normalizer\n",
    ")\n",
    "\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform = normalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As this is a binary classification problem where we only want to identify whether an image is a ship or not, we can set the labels that are \"ship\" to true. We set all other labels to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = np.array([label for _, label in transformed_cifar10_train_val])\n",
    "train_labels = np.array(train_labels == category_index).astype(int)\n",
    "\n",
    "test_labels = np.array([label for _, label in transformed_cifar10_test])\n",
    "test_labels = np.array(test_labels == category_index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Splitting the training and validation set randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({8: 4486,\n",
       "         7: 4486,\n",
       "         1: 4512,\n",
       "         6: 4510,\n",
       "         3: 4516,\n",
       "         5: 4487,\n",
       "         4: 4499,\n",
       "         2: 4483,\n",
       "         0: 4518,\n",
       "         9: 4503})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = len(transformed_cifar10_train_val)-n_val\n",
    "\n",
    "transformed_cifar10_train_split, transformed_cifar10_val_split = random_split(\n",
    "    transformed_cifar10_train_val,\n",
    "    [n_train, n_val],\n",
    "\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "print(\"Size of the train dataset:        \", len(transformed_cifar10_train_split))\n",
    "print(\"Size of the validation dataset:   \", len(transformed_cifar10_val_split))\n",
    "print(\"Size of the test dataset:         \", len(transformed_cifar10_test))\n",
    "\n",
    "Counter([label for _, label in transformed_cifar10_train_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Choosing a pre-trained CNN model: we chose ResNet18, which is not trained on Cifar-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loading pre-trained ResNet18 model and modifying the last layer. We are doing binary classification, so we think we only need one node in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\inf265\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\Anaconda3\\envs\\inf265\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use Binary Cross Entropy as it should be suitable for binary classification problems (add reasoning and explanation). We use nn.BCEWithLogitsLoss() as it combines the sigmoid activation function and the BCE into a single class.\n",
    "\n",
    "The optimizer we use is Adam, and we will begin with a learning rate of 0.001, just because it is a commonly used learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m----> 4\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49madam(model\u001b[39m.\u001b[39mparameters(),  lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'adam'"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.adam(model.parameters(),  lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
