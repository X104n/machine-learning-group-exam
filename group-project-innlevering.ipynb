{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#TO Encode, Scale and split data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#The Models we are going to use\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#To make a print_score function\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data and preprocess it\n",
    "dataset =  pd.read_csv(\"agaricus-lepiota.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Want to rename the colums, so its easier to evaluate\n",
    "dataset.columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "                   'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "                   'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type',\n",
    "                   'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Label Encoding, so that the cells contain corresponding number for the character, and replaces it.\n",
    "Encoder = LabelEncoder()\n",
    "for col in dataset.columns:\n",
    "    dataset[col] = Encoder.fit_transform(dataset[col])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "#dataset.info() #Shows that every row has non-null values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Want to get a quick overview of how many mushrooms are poisonous or not. We can see that there are roughly the same\n",
    "# amount of poisonous and edible\n",
    "#sns.countplot(x='class', data=dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we want to generate a heatmap to visualize the correlation between the attributes.\n",
    "\n",
    "We want to use the heatmap to identify which features are strongly correlated with the target variable ('class') and with each other. This can help us determine which features we should drop in order to create a more accurate model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sns.heatmap(dataset.corr())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the heatmap show above, white cells shows a high correlation, and if the attribute is strongly correlated with the target variable 'class' it is not a useful feature for the prediction and should be dropped. Also, we can see that some attributes are highly correleted with eachother, we would need to choose to drop one of them to avoid multicollinearity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Need to drop some coloms because of the logical rules in the dataset.\n",
    "#This is features that are the most indicative, and we would therefore drop these before running the models.\n",
    "drop_features = ['odor', 'spore-print-color', 'habitat', 'stalk-shape','gill-size','gill-spacing','bruises',\n",
    "                 'gill-color','stalk-root','ring-type','stalk-surface-below-ring','stalk-surface-above-ring',\n",
    "                 'population', 'cap-color']\n",
    "dataset = dataset.drop(drop_features, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X=dataset.drop('class',axis=1) #Predictors 'p'=\"class\", This is what we are trying to predict.\n",
    "y=dataset['class'] #Response, the data we have to work with, is then the rest of the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scalar = StandardScaler()\n",
    "#X = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=38)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to use KNN, DecisionTreeclassifier and RandomForrestClassifier.\n",
    "The reason for each is:\n",
    "\n",
    "We want to use KNN because its a \"lazy\" algorithm that is easy to implement and interpret. KNN is very efficient and accurate for small datasets, and we think therefore this is a good fit for our dataset. We are going to use gridsearch to get the most optimal number of neighbours.\n",
    "\n",
    "Random Forrest is a useful algorithm for the mushroom dataset because it can handle large amounts of data and noisy or missing data. Since the mushroom dataset contains many different attributes the Random Forest can be used to identify which attributes are most important for classification by creating multiple decision trees and combining their results.\n",
    "\n",
    "Decision Tree is well-suited for the mushroom dataset since it can handle non-linear classification problems and can capture the complex interactions between the features. Additionally, decision trees are easy to interpret, which can be useful for understanding which features are most important for classification.\n",
    "\n",
    "Overall, the combination of these three algorithms provides a good balance between accuracy, interpretability, and robustness for the classification task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Grid search to find best parameter\n",
    "knn_gs = KNN()\n",
    "param_grid = {'n_neighbors': range(1, 31)}  # Define the range of neighbors to test\n",
    "grid_search = GridSearchCV(knn_gs, param_grid, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "best_param = grid_search.best_params_['n_neighbors']# get the best parameter\n",
    "\n",
    "#KNN\n",
    "knn = KNN(n_neighbors=best_param)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "#RandomForrestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#A function to print a classification report for the different models.\n",
    "def print_score(classifier,X_train,y_train,X_test,y_test, name):\n",
    "    print(\"Training results for\", name, \":\\n\")\n",
    "    print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print_score(knn,X_train,y_train,X_test,y_test,\"KNN\")\n",
    "print_score(dtc,X_train,y_train,X_test,y_test,\"DTC\")\n",
    "print_score(rfc,X_train,y_train,X_test,y_test,\"RFC\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Our best performing model is the RandomForrest classifier, as the average score is 84%. We would still not trust the model as there is still 20% that is wrong, and that can be dangerous. The dataset was originally sufficient because it would give a score of 100% for all three models. To really test the models we would need to decrease the numbers of colums and avoid the logical rules of the dataset.\n",
    "\n",
    "The decision tree classifier is a tree-like model that splits the data based on the most significant feature until a prediction can be made. We think the random forrest classifier perform best because it is an ensemble of decision trees, where each tree makes a prediction and the final prediction is made by combining the predictions of all trees. Therefore, the RFC is even more precise than the dtc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This first part of the task I will be evaluating my preprocessing. Before starting with the real Sentiment Analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# Extracts only the text part of the document.\n",
    "def data_processing(file_name) -> list:\n",
    "    message_data = []\n",
    "    with open(f'{file_name}', \"r\") as training_data:\n",
    "        train_array = json.load(training_data)\n",
    "\n",
    "        #Train_array is a array with dictionaries, each having three elements each.\n",
    "        for category in train_array:\n",
    "            #category is the index of the list with the dictionary.\n",
    "\n",
    "            message_data.append(category[\"text\"])\n",
    "\n",
    "    return message_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I ask my group leader who gave a good advice when it comes to tokenizing data and removing the stopwords.\n",
    "\n",
    "He adviced me to sanitize the data by removing the tokens while tokenizing them, which was a smart way of avoiding unnecessery high amount of list accesses. The function sentance_sanitized_tokening() takes the list with full sentances, and then iterates through each tokenized word in the sentance, in order to check if it exists in dummyWords which is a english stopword list from nltk, if the word exists in the list it just goes to the next iteration/word. If it ain't then it adds it to the list of words that is to be added to the output list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentance tokenizing and stopword removing function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentance_sanitized_tokening(data: dict) -> list:\n",
    "    tokenized_list = []\n",
    "    dummyWords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # data_part = [\"text\", \"label\"]\n",
    "    for data_part in data:\n",
    "        sentance, label = data_part\n",
    "        tokened_sentance = nltk.word_tokenize(sentance)\n",
    "\n",
    "        # Each sentance has it's own filtered_tokens list containing the words that is not stopwords.\n",
    "        filtered_tokens = []\n",
    "\n",
    "        for word in tokened_sentance:\n",
    "            assert type(word)==str, \"Words inside the dictionary passed to tokenize(data) is not of type string!!\"\n",
    "            # I considered using remove() function but that would have a very high impact on programs runtime which I prefere to avoid.\n",
    "\n",
    "            if word not in dummyWords:\n",
    "                filtered_tokens.append(word)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # When all the words from the sentance that were not stopwords was added then append them to the output (tokenized_list)\n",
    "        tokenized_list.append([filtered_tokens, label])\n",
    "\n",
    "    return tokenized_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing I considered was weather the symbol \"!\" should be removed. It may be used to help characterized Negative words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Task 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import random_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "category_index_manual = 8\n",
    "n_val = 5000\n",
    "\n",
    "data_path_manual = '/cifar-10-batches-py'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for loading the Cifar10 dataset.\n",
    "\n",
    "The method will have to be run twice. The first time we load it as tensors and split the set between training, validation, and testing. We only want to identify one type of image (ships), so we get the labels of all the images and set all to \"false\" if they are not ships.\n",
    "\n",
    "After running the method for the first time we get create a normalizer from the std and mean of the images. The method is then ran for a second time with the normalizer as the preprocessor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def load_dataset(n_val, category_index, data_path, preprocessor):\n",
    "    transformed_cifar10_train_val = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform = preprocessor\n",
    "    )\n",
    "\n",
    "    transformed_cifar10_test = datasets.CIFAR10(\n",
    "        data_path,\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform = preprocessor\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    n_train = len(transformed_cifar10_train_val)-n_val\n",
    "\n",
    "    transformed_cifar10_train_split, transformed_cifar10_val_split = random_split(\n",
    "        transformed_cifar10_train_val,\n",
    "        [n_train, n_val],\n",
    "\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(transformed_cifar10_train_split))\n",
    "    print(\"Size of the validation dataset:   \", len(transformed_cifar10_val_split))\n",
    "    print(\"Size of the test dataset:         \", len(transformed_cifar10_test))\n",
    "\n",
    "load_dataset(n_val, category_index_manual, data_path_manual, preprocessor=transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels = np.array(transformed_cifar10_train_val)\n",
    "train_labels = np.array(train_labels == category_index).astype(int)\n",
    "\n",
    "test_labels = np.array(transformed_cifar10_test)\n",
    "test_labels = np.array(test_labels == category_index).astype(int)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in transformed_cifar10_train_val])\n",
    "\n",
    "normalizer = transforms.Normalize(mean = imgs.mean(dim=(0, 2, 3)), std = imgs.std(dim=(0, 2, 3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last layer for binary classification\n",
    "# Legg til markdown om at vi løser det som binary classification\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,1)\n",
    "\n",
    "# \"Remember to use a suitable loss function like Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss) and an optimizer like Adam or SGD for training.\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Legge til spørsmål om vi burde bruke SGD, Adam, RMSprop eller noe annet\n",
    "# Legge til spørsmål om vi burde endre på learning raten, eller i alle fall forklare hvorfor vi bruker den vi bruker\n",
    "optimizer = optim.adam(model.parameters(),  lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
